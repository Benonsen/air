{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308e4a14-624a-4d31-8e9b-d253c65c376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# prepare data for task\n",
    "\n",
    "# 1) Download the collection set from the Gitlab repository: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b\n",
    "# 2) Drag and drop the downloaded file to the \"Files\" section (left vertical menu on Colab)\n",
    "# 3) Modify the path to your local file path\n",
    "PATH_COLLECTION_DATA = 'subtask4b_collection_data.pkl' #MODIFY PATH\n",
    "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)\n",
    "\n",
    "PATH_QUERY_TRAIN_DATA = 'subtask4b_query_tweets_train.tsv' #MODIFY PATH\n",
    "PATH_QUERY_DEV_DATA = 'subtask4b_query_tweets_dev.tsv' #MODIFY PATH\n",
    "PATH_QUERY_DEV_TEST = 'subtask4b_query_tweets_test.tsv' #MODIFY PATH\n",
    "\n",
    "df_query_train = pd.read_csv(PATH_QUERY_TRAIN_DATA, sep = '\\t')\n",
    "df_query_dev = pd.read_csv(PATH_QUERY_DEV_DATA, sep = '\\t')\n",
    "df_query_test = pd.read_csv(PATH_QUERY_DEV_TEST, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fcebc4-f89d-4652-b2ef-bb14c3a3bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "\n",
    "tokenizer_cross_encoder = AutoTokenizer.from_pretrained(model_name)\n",
    "cross_encoder = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eb3543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block of code creates the dataset for the neural ranking\n",
    "\n",
    "df_merged = pd.merge(df_query_train, df_collection, on='cord_uid', how='inner')\n",
    "\n",
    "def format_paper(row):\n",
    "    return f\"{row['title'].strip()} [SEP] {row['abstract'].strip()}\"\n",
    "\n",
    "df_merged['paper_text'] = df_merged.apply(format_paper, axis=1)\n",
    "\n",
    "# positive examples\n",
    "positive_samples = [\n",
    "    {\"tweet\": row['tweet_text'], \"paper\": row['paper_text'], \"label\": 1.0}\n",
    "    for _, row in df_merged.iterrows()\n",
    "]\n",
    "\n",
    "# negative examples\n",
    "all_paper_ids = set(df_collection['cord_uid'])\n",
    "paper_text_lookup = {\n",
    "    row['cord_uid']: format_paper(row) for _, row in df_collection.iterrows()\n",
    "}\n",
    "\n",
    "negative_samples = []\n",
    "# for every positive sample, create 3 negate samples\n",
    "# a negative sample is one where the tweet does not match the paper\n",
    "for _ in range(3):\n",
    "    for _, row in df_query_train.iterrows():\n",
    "        tweet = row['tweet_text']\n",
    "        correct_id = row['cord_uid']\n",
    "        negative_id = random.choice(list(all_paper_ids - {correct_id}))\n",
    "        negative_text = paper_text_lookup[negative_id]\n",
    "        negative_samples.append({\n",
    "            \"tweet\": tweet, \"paper\": negative_text, \"label\": 0.0\n",
    "        })\n",
    "\n",
    "all_samples = positive_samples + negative_samples\n",
    "random.shuffle(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf421976",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer_cross_encoder, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer_cross_encoder = tokenizer_cross_encoder\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        encoded = self.tokenizer_cross_encoder(\n",
    "            item[\"tweet\"],\n",
    "            item[\"paper\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(item[\"label\"], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebfcc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TweetPaperDataset(all_samples, tokenizer_cross_encoder)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine-tuned-cross-encoder\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"no\", \n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=cross_encoder,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f956558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cross_encoder, 'ranking_ft.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ee6a49-be4f-4f45-924c-aacb2c4e3492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "class ClaimSourceDataset(Dataset):\n",
    "    def __init__(self, df, collection_df, tokenizer, max_len=512):\n",
    "        self.df = df\n",
    "        self.collection = collection_df.set_index('cord_uid')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        tweet = row['tweet_text']\n",
    "        paper_id = row['cord_uid']\n",
    "        paper_row = self.collection.loc[paper_id]\n",
    "        paper = f\"{paper_row['title']} {paper_row['abstract']}\"\n",
    "\n",
    "        tweet_enc = self.tokenizer(tweet, truncation=True, padding='max_length', max_length=self.max_len, return_tensors=\"pt\")\n",
    "        paper_enc = self.tokenizer(paper, truncation=True, padding='max_length', max_length=self.max_len, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            'tweet_input_ids': tweet_enc['input_ids'].squeeze(),\n",
    "            'tweet_attention_mask': tweet_enc['attention_mask'].squeeze(),\n",
    "            'paper_input_ids': paper_enc['input_ids'].squeeze(),\n",
    "            'paper_attention_mask': paper_enc['attention_mask'].squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fb2056-bca3-4674-9b90-afd5a688a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class EncoderModel(nn.Module):\n",
    "    def __init__(self, model_name=\"allenai/scibert_scivocab_uncased\", local = True):\n",
    "        super().__init__()\n",
    "        if local: \n",
    "            self.encoder = torch.load('final_scibert_512_5.pt')\n",
    "        else: \n",
    "            self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "    def forward(self, tweet_ids, tweet_mask, paper_ids, paper_mask):\n",
    "        tweet_vec = self.encoder(tweet_ids, attention_mask=tweet_mask).last_hidden_state[:, 0]\n",
    "        paper_vec = self.encoder(paper_ids, attention_mask=paper_mask).last_hidden_state[:, 0]\n",
    "        return tweet_vec, paper_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6231747d-22bf-4f14-8ff0-029c62fe4e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029a6989-16ef-4ce5-a797-2925ba85c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "def encode_papers(model, df_collection, tokenizer, batch_size=32):\n",
    "    model.eval()\n",
    "    paper_texts = df_collection.apply(lambda row: f\"{row['title']} {row['abstract']}\", axis=1).tolist()\n",
    "    paper_ids = df_collection['cord_uid'].tolist()\n",
    "\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(paper_texts), batch_size)):\n",
    "            batch = paper_texts[i:i+batch_size]\n",
    "            encodings = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=512, return_overflowing_tokens=False)\n",
    "            input_ids = encodings['input_ids'].to(device)\n",
    "            attention_mask = encodings['attention_mask'].to(device)\n",
    "            # remove one .encoder if you download the model and do not use the local weight file\n",
    "            vecs = model.encoder.encoder(input_ids, attention_mask=attention_mask).last_hidden_state[:, 0]\n",
    "            all_embeddings.append(vecs.cpu().numpy())\n",
    "\n",
    "    return paper_ids, np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3cd7a-bb9d-456d-a327-2c47a749a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert = EncoderModel()\n",
    "paper_ids, paper_embeddings = encode_papers(bert, df_collection, tokenizer)\n",
    "\n",
    "faiss_index = faiss.IndexFlatIP(paper_embeddings.shape[1])\n",
    "faiss_index.add(paper_embeddings)\n",
    "\n",
    "paper_id_map = {i: pid for i, pid in enumerate(paper_ids)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a290f-5d78-4d1d-a100-830b7eda3071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "def retrieve(model, cross_encoder, df_query_dev, tokenizer, tokenizer_cross_encoder, faiss_index, paper_id_map, paper_text_lookup, topk=10):\n",
    "    model.eval()\n",
    "    cross_encoder.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(df_query_dev['tweet_text']):\n",
    "\n",
    "            enc = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "            tweet_vec = model.encoder.encoder(enc['input_ids'], attention_mask=enc['attention_mask']).last_hidden_state[:, 0]\n",
    "            tweet_vec = F.normalize(tweet_vec, dim=1).cpu().numpy()\n",
    "\n",
    "\n",
    "            D, I = faiss_index.search(tweet_vec, 10)\n",
    "            candidate_ids = [paper_id_map[idx] for idx in I[0]]\n",
    "            candidate_texts = [paper_text_lookup[pid] for pid in candidate_ids]\n",
    "\n",
    "            inputs = tokenizer_cross_encoder(\n",
    "                [text] * len(candidate_texts),\n",
    "                candidate_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            outputs = cross_encoder(**inputs)\n",
    "\n",
    "            scores = outputs.logits.squeeze().tolist()  # shape: (topk,)\n",
    "\n",
    "            reranked = sorted(zip(candidate_ids, scores), key=lambda x: x[1], reverse=True)\n",
    "            top_docs = [doc_id for doc_id, _ in reranked[:10]]\n",
    "            predictions.append(top_docs)\n",
    "\n",
    "    df_query_dev['dense_topk'] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c1cf12-bbe8-4e20-b532-25ccec0f2cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_text_lookup = {\n",
    "    row['cord_uid']: format_paper(row) for _, row in df_collection.iterrows()\n",
    "}\n",
    "retrieve(bert, cross_encoder, df_query_test, tokenizer, tokenizer_cross_encoder, faiss_index, paper_id_map, paper_text_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601673d0-e1ca-4b41-b471-3640d64f0538",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_test['preds'] = df_query_test['dense_topk'].apply(lambda x: x[:5])\n",
    "df_query_test[['post_id', 'preds']].to_csv('predictions_final_ranking_646.tsv', index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d991caa6-1ff9-4496-9a45-eeb39a7e787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_text_lookup = {\n",
    "    row['cord_uid']: format_paper(row) for _, row in df_collection.iterrows()\n",
    "}\n",
    "\n",
    "# Evaluate retrieved candidates using MRR@k\n",
    "def get_performance_mrr(data, col_gold, col_pred, list_k = [1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(lambda x: (1/([i for i in x[col_pred][:k]].index(x[col_gold]) + 1) if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "        #performances.append(data[\"in_topx\"].mean())\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance\n",
    "\n",
    "# Evaluate\n",
    "retrieve(bert, cross_encoder, df_query_dev, tokenizer, tokenizer_cross_encoder, faiss_index, paper_id_map, paper_text_lookup)\n",
    "results_test = get_performance_mrr(df_query_dev, 'cord_uid', 'dense_topk')\n",
    "print(\"MRR Results:\", results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e82cb6-8993-4890-b627-70c131269f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20, weak cross encoder\n",
    "# MRR Results: {1: 0.5914285714285714, 5: 0.6446071428571429, 10: 0.6499339569160998}\n",
    "# top 20, loaded cross encoder\n",
    "# MRR Results: {1: 0.27714285714285714, 5: 0.37324999999999997, 10: 0.3931893424036282}\n",
    "# not finetuned, top 10\n",
    "# MRR Results: {1: 0.585, 5: 0.6383928571428571, 10: 0.6441590136054423\n",
    "\n",
    "# 20, little bit more ft, just positive. \n",
    "#MRR Results: {1: 0.595, 5: 0.6461071428571429, 10: 0.651310941043084}\n",
    "\n",
    "# 20 reranking big one\n",
    "# MRR Results: {1: 0.5057142857142857, 5: 0.5909999999999999, 10: 0.598437074829932}\n",
    "\n",
    "\n",
    "# MRR Results: {1: 0.32571428571428573, 5: 0.4390476190476191, 10: 0.4537562358276644}\n",
    "\n",
    "\n",
    "# just positive 1 epoch, 512, 10\n",
    "# MRR Results: {1: 0.5907142857142857, 5: 0.6467619047619048, 10: 0.6523293650793651}\n",
    "# just postive 1 epoch, 512, 15\n",
    "# MRR Results: {1: 0.5835714285714285, 5: 0.6447142857142857, 10: 0.6504889455782313}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
