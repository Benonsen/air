{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "308e4a14-624a-4d31-8e9b-d253c65c376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "PATH_COLLECTION_DATA = 'subtask4b_collection_data.pkl' #MODIFY PATH\n",
    "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbc61f7e-047f-431e-9225-6a11353b464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_QUERY_TRAIN_DATA = 'subtask4b_query_tweets_train.tsv' #MODIFY PATH\n",
    "PATH_QUERY_DEV_DATA = 'subtask4b_query_tweets_dev.tsv' #MODIFY PATH\n",
    "PATH_QUERY_DEV_TEST = 'subtask4b_query_tweets_test.tsv' #MODIFY PATH\n",
    "df_query_train = pd.read_csv(PATH_QUERY_TRAIN_DATA, sep = '\\t')\n",
    "df_query_dev = pd.read_csv(PATH_QUERY_DEV_DATA, sep = '\\t')\n",
    "df_query_test = pd.read_csv(PATH_QUERY_DEV_TEST, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04fcebc4-f89d-4652-b2ef-bb14c3a3bbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 14:08:56.513084: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746799736.554421    4840 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746799736.567207    4840 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746799736.593645    4840 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746799736.593665    4840 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746799736.593668    4840 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746799736.593672    4840 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-09 14:08:56.601954: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_def.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_avx2.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_core.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_lp64.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_thread.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2387' max='4821' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2387/4821 47:34 < 48:33, 0.84 it/s, Epoch 1.48/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>14.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.183300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.119900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.086200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.065300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.046800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.046500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.038700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.028600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.029600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.023100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.025500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.030400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.021200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.024100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.032300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.024500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.014800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.016600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.009400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.015200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 99\u001b[0m\n\u001b[1;32m     92\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     93\u001b[0m     model\u001b[38;5;241m=\u001b[39mcross_encoder,\n\u001b[1;32m     94\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     95\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Start fine-tuning\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:2565\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2566\u001b[0m ):\n\u001b[1;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2569\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "model_name = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\n",
    "#model_name = \"Alibaba-NLP/gte-reranker-modernbert-base\"\n",
    "tokenizer_cross_encoder = AutoTokenizer.from_pretrained(model_name)\n",
    "cross_encoder = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "# --- DATA PREPARATION ---\n",
    "# Merge metadata into positive pairs\n",
    "df_merged = pd.merge(df_query_train, df_collection, on='cord_uid', how='inner')\n",
    "\n",
    "def format_paper(row):\n",
    "    return f\"{row['title'].strip()} [SEP] {row['abstract'].strip()}\"\n",
    "\n",
    "df_merged['paper_text'] = df_merged.apply(format_paper, axis=1)\n",
    "\n",
    "# Positive examples\n",
    "positive_samples = [\n",
    "    {\"tweet\": row['tweet_text'], \"paper\": row['paper_text'], \"label\": 1.0}\n",
    "    for _, row in df_merged.iterrows()\n",
    "]\n",
    "\n",
    "# Negative examples\n",
    "all_paper_ids = set(df_collection['cord_uid'])\n",
    "paper_text_lookup = {\n",
    "    row['cord_uid']: format_paper(row) for _, row in df_collection.iterrows()\n",
    "}\n",
    "\n",
    "negative_samples = []\n",
    "for _ in range(3):\n",
    "    for _, row in df_query_train.iterrows():\n",
    "        tweet = row['tweet_text']\n",
    "        correct_id = row['cord_uid']\n",
    "        negative_id = random.choice(list(all_paper_ids - {correct_id}))\n",
    "        negative_text = paper_text_lookup[negative_id]\n",
    "        negative_samples.append({\n",
    "            \"tweet\": tweet, \"paper\": negative_text, \"label\": 0.0\n",
    "        })\n",
    "\n",
    "# Combine and shuffle\n",
    "all_samples = positive_samples + negative_samples\n",
    "random.shuffle(all_samples)\n",
    "\n",
    "# --- DATASET CLASS ---\n",
    "class TweetPaperDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer_cross_encoder, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer_cross_encoder = tokenizer_cross_encoder\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        encoded = self.tokenizer_cross_encoder(\n",
    "            item[\"tweet\"],\n",
    "            item[\"paper\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(item[\"label\"], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Create Dataset\n",
    "train_dataset = TweetPaperDataset(all_samples, tokenizer_cross_encoder)\n",
    "\n",
    "# --- TRAINING ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine-tuned-cross-encoder\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"no\", \n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=cross_encoder,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54ee6a49-be4f-4f45-924c-aacb2c4e3492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "class ClaimSourceDataset(Dataset):\n",
    "    def __init__(self, df, collection_df, tokenizer, max_len=512):\n",
    "        self.df = df\n",
    "        self.collection = collection_df.set_index('cord_uid')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        tweet = row['tweet_text']\n",
    "        paper_id = row['cord_uid']\n",
    "        paper_row = self.collection.loc[paper_id]\n",
    "        paper = f\"{paper_row['title']} {paper_row['abstract']}\"\n",
    "\n",
    "        tweet_enc = self.tokenizer(tweet, truncation=True, padding='max_length', max_length=self.max_len, return_tensors=\"pt\")\n",
    "        paper_enc = self.tokenizer(paper, truncation=True, padding='max_length', max_length=self.max_len, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            'tweet_input_ids': tweet_enc['input_ids'].squeeze(),\n",
    "            'tweet_attention_mask': tweet_enc['attention_mask'].squeeze(),\n",
    "            'paper_input_ids': paper_enc['input_ids'].squeeze(),\n",
    "            'paper_attention_mask': paper_enc['attention_mask'].squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63fb2056-bca3-4674-9b90-afd5a688a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class DualEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"allenai/scibert_scivocab_uncased\"):\n",
    "        super().__init__()\n",
    "        if True: \n",
    "            self.encoder = torch.load('final_scibert_512_5_r.pt')\n",
    "        else: \n",
    "            self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "    def forward(self, tweet_ids, tweet_mask, paper_ids, paper_mask):\n",
    "        tweet_vec = self.encoder(tweet_ids, attention_mask=tweet_mask).last_hidden_state[:, 0]\n",
    "        paper_vec = self.encoder(paper_ids, attention_mask=paper_mask).last_hidden_state[:, 0]\n",
    "        return tweet_vec, paper_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6231747d-22bf-4f14-8ff0-029c62fe4e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "029a6989-16ef-4ce5-a797-2925ba85c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "def encode_papers(model, df_collection, tokenizer, batch_size=32):\n",
    "    model.eval()\n",
    "    paper_texts = df_collection.apply(lambda row: f\"{row['title']} {row['abstract']}\", axis=1).tolist()\n",
    "    paper_ids = df_collection['cord_uid'].tolist()\n",
    "\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(paper_texts), batch_size)):\n",
    "            batch = paper_texts[i:i+batch_size]\n",
    "            encodings = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=256, return_overflowing_tokens=False)\n",
    "            input_ids = encodings['input_ids'].to(device)\n",
    "            attention_mask = encodings['attention_mask'].to(device)\n",
    "            # remove one .encoder if you download the model and do not use the local weight file\n",
    "            vecs = model.encoder.encoder(input_ids, attention_mask=attention_mask).last_hidden_state[:, 0]\n",
    "            all_embeddings.append(vecs.cpu().numpy())\n",
    "\n",
    "    return paper_ids, np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c405e72-18f1-4bab-9d62-9a039f7164fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cross_encoder, 'ranking.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8b3cd7a-bb9d-456d-a327-2c47a749a7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb90e34a44c646b9bbb5600ababf88e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert = DualEncoder()\n",
    "paper_ids, paper_embeddings = encode_papers(bert, df_collection, tokenizer)\n",
    "\n",
    "faiss_index = faiss.IndexFlatIP(paper_embeddings.shape[1])\n",
    "faiss_index.add(paper_embeddings)\n",
    "\n",
    "paper_id_map = {i: pid for i, pid in enumerate(paper_ids)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f4a290f-5d78-4d1d-a100-830b7eda3071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "def retrieve(model, cross_encoder, df_query_dev, tokenizer, tokenizer_cross_encoder, faiss_index, paper_id_map, paper_text_lookup, topk=10):\n",
    "    model.eval()\n",
    "    cross_encoder.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(df_query_dev['tweet_text']):\n",
    "\n",
    "            enc = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "            tweet_vec = model.encoder.encoder(enc['input_ids'], attention_mask=enc['attention_mask']).last_hidden_state[:, 0]\n",
    "            tweet_vec = F.normalize(tweet_vec, dim=1).cpu().numpy()\n",
    "\n",
    "\n",
    "            D, I = faiss_index.search(tweet_vec, 20)\n",
    "            candidate_ids = [paper_id_map[idx] for idx in I[0]]\n",
    "            candidate_texts = [paper_text_lookup[pid] for pid in candidate_ids]\n",
    "\n",
    "            # Step 3: Use cross-encoder to score the pairs\n",
    "            inputs = tokenizer_cross_encoder(\n",
    "                [text] * len(candidate_texts),\n",
    "                candidate_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            outputs = cross_encoder(**inputs)\n",
    "\n",
    "            scores = outputs.logits.squeeze().tolist()  # shape: (topk,)\n",
    "\n",
    "            reranked = sorted(zip(candidate_ids, scores), key=lambda x: x[1], reverse=True)\n",
    "            top_docs = [doc_id for doc_id, _ in reranked[:10]]\n",
    "            predictions.append(top_docs)\n",
    "\n",
    "    df_query_dev['dense_topk'] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5c1cf12-bbe8-4e20-b532-25ccec0f2cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e836314fdcf0487d969b6e8f29494ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1446 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paper_text_lookup = {\n",
    "    row['cord_uid']: format_paper(row) for _, row in df_collection.iterrows()\n",
    "}\n",
    "retrieve(bert, cross_encoder, df_query_test, tokenizer, tokenizer_cross_encoder, faiss_index, paper_id_map, paper_text_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "601673d0-e1ca-4b41-b471-3640d64f0538",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_test['preds'] = df_query_test['dense_topk'].apply(lambda x: x[:5])\n",
    "df_query_test[['post_id', 'preds']].to_csv('predictions_final_ranking.tsv', index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d991caa6-1ff9-4496-9a45-eeb39a7e787f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b800b3a8404462b86b8aeebb7885f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR Results: {1: 0.5057142857142857, 5: 0.5909999999999999, 10: 0.598437074829932}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate retrieved candidates using MRR@k\n",
    "def get_performance_mrr(data, col_gold, col_pred, list_k = [1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(lambda x: (1/([i for i in x[col_pred][:k]].index(x[col_gold]) + 1) if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "        #performances.append(data[\"in_topx\"].mean())\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance\n",
    "\n",
    "# Evaluate\n",
    "retrieve(bert, cross_encoder, df_query_dev, tokenizer, tokenizer_cross_encoder, faiss_index, paper_id_map, paper_text_lookup)\n",
    "results_test = get_performance_mrr(df_query_dev, 'cord_uid', 'dense_topk')\n",
    "print(\"MRR Results:\", results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e82cb6-8993-4890-b627-70c131269f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20, weak cross encoder\n",
    "# MRR Results: {1: 0.5914285714285714, 5: 0.6446071428571429, 10: 0.6499339569160998}\n",
    "# top 20, loaded cross encoder\n",
    "# MRR Results: {1: 0.27714285714285714, 5: 0.37324999999999997, 10: 0.3931893424036282}\n",
    "# not finetuned, top 10\n",
    "# MRR Results: {1: 0.585, 5: 0.6383928571428571, 10: 0.6441590136054423\n",
    "\n",
    "# 20, little bit more ft, just positive. \n",
    "#MRR Results: {1: 0.595, 5: 0.6461071428571429, 10: 0.651310941043084}\n",
    "\n",
    "# 20 reranking big one\n",
    "# MRR Results: {1: 0.5057142857142857, 5: 0.5909999999999999, 10: 0.598437074829932}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
