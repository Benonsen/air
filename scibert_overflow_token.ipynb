{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "035b203c-b08b-4a09-9a9a-42554f4d814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class ClaimSourceDataset(Dataset):\n",
    "    def __init__(self, df, collection_df, tokenizer, max_len=256):\n",
    "        self.samples = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "        paper_lookup = collection_df.set_index('cord_uid')\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            tweet = row['tweet_text']\n",
    "            paper_id = row['cord_uid']\n",
    "\n",
    "            if paper_id not in paper_lookup.index:\n",
    "                continue\n",
    "\n",
    "            paper_row = paper_lookup.loc[paper_id]\n",
    "            paper = f\"{paper_row['title']} {paper_row['abstract']}\"\n",
    "\n",
    "            paper_tokenized = tokenizer(\n",
    "                paper,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\",\n",
    "                return_overflowing_tokens=True\n",
    "            )\n",
    "\n",
    "            tweet_tokenized = tokenizer(\n",
    "                tweet,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\",\n",
    "                return_overflowing_tokens=True\n",
    "            )\n",
    "\n",
    "            # Handle multiple overflow chunks (if any) — flat storage\n",
    "            for i in range(len(paper_tokenized['input_ids'])):\n",
    "                for j in range(len(tweet_tokenized['input_ids'])):\n",
    "                    self.samples.append({\n",
    "                        'tweet_input_ids': tweet_tokenized['input_ids'][j].squeeze(0),\n",
    "                        'tweet_attention_mask': tweet_tokenized['attention_mask'][j].squeeze(0),\n",
    "                        'paper_input_ids': paper_tokenized['input_ids'][i].squeeze(0),\n",
    "                        'paper_attention_mask': paper_tokenized['attention_mask'][i].squeeze(0)\n",
    "                    })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e5e995-d1f4-4f8c-b594-8e535e1a4fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_name=\"scibert_scivocab_uncased\"):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "\n",
    "    def forward(self, tweet_ids, tweet_mask, paper_ids, paper_mask):\n",
    "        tweet_vec = self.encoder(tweet_ids, attention_mask=tweet_mask).last_hidden_state[:, 0]\n",
    "        paper_vec = self.encoder(paper_ids, attention_mask=paper_mask).last_hidden_state[:, 0]\n",
    "        return tweet_vec, paper_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c39d5c8-27b1-494e-8aad-ea688fa1e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def contrastive_loss(tweet_vecs, paper_vecs, temperature=0.05):\n",
    "    tweet_vecs = F.normalize(tweet_vecs, dim=1)\n",
    "    paper_vecs = F.normalize(paper_vecs, dim=1)\n",
    "\n",
    "    logits = torch.matmul(tweet_vecs, paper_vecs.T) / temperature\n",
    "    labels = torch.arange(len(tweet_vecs)).to(tweet_vecs.device)\n",
    "    return F.cross_entropy(logits, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7152b6-ff4a-48d7-bede-07f079d252fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# prepare data for task\n",
    "\n",
    "# 1) Download the collection set from the Gitlab repository: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b\n",
    "# 2) Drag and drop the downloaded file to the \"Files\" section (left vertical menu on Colab)\n",
    "# 3) Modify the path to your local file path\n",
    "PATH_COLLECTION_DATA = 'subtask4b_collection_data.pkl' #MODIFY PATH\n",
    "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)\n",
    "\n",
    "PATH_QUERY_TRAIN_DATA = 'subtask4b_query_tweets_train.tsv' #MODIFY PATH\n",
    "PATH_QUERY_DEV_DATA = 'subtask4b_query_tweets_dev.tsv' #MODIFY PATH\n",
    "PATH_QUERY_DEV_TEST = 'subtask4b_query_tweets_test.tsv' #MODIFY PATH\n",
    "\n",
    "df_query_train = pd.read_csv(PATH_QUERY_TRAIN_DATA, sep = '\\t')\n",
    "df_query_dev = pd.read_csv(PATH_QUERY_DEV_DATA, sep = '\\t')\n",
    "df_query_test = pd.read_csv(PATH_QUERY_DEV_TEST, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280be91d-7fab-46d6-86b4-6445543bf857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', return_overflowing_tokens=True)\n",
    "model = Encoder().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "train_dataset = ClaimSourceDataset(df_query_train, df_collection, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16 , shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "346cbd24-b072-450c-9510-3bde88c0c989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1587/1587 [36:48<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1587/1587 [36:11<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.9617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1587/1587 [35:55<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.8946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1587/1587 [36:01<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.8783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1587/1587 [36:42<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.8625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        tweet_ids = batch['tweet_input_ids'].to(device)\n",
    "        tweet_mask = batch['tweet_attention_mask'].to(device)\n",
    "        paper_ids = batch['paper_input_ids'].to(device)\n",
    "        paper_mask = batch['paper_attention_mask'].to(device)\n",
    "\n",
    "        tweet_vecs, paper_vecs = model(tweet_ids, tweet_mask, paper_ids, paper_mask)\n",
    "\n",
    "        loss = contrastive_loss(tweet_vecs, paper_vecs)\n",
    "   \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0fdc2e8-b9e7-4846-a7ef-3e0a0a1c15f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def encode_papers(model, df_collection, tokenizer, batch_size=8, max_len=256):\n",
    "    model.eval()\n",
    "    paper_texts = df_collection.apply(lambda row: f\"{row['title']} {row['abstract']}\", axis=1).tolist()\n",
    "    paper_ids = df_collection['cord_uid'].tolist()\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_paper_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(paper_texts), batch_size):\n",
    "            batch_texts = paper_texts[i:i+batch_size]\n",
    "            batch_ids = paper_ids[i:i+batch_size]\n",
    "\n",
    "            encodings = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_overflowing_tokens=True,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=max_len\n",
    "            )\n",
    "\n",
    "            input_ids = encodings['input_ids'].to(device)\n",
    "            attention_mask = encodings['attention_mask'].to(device)\n",
    "            overflow_map = encodings['overflow_to_sample_mapping']\n",
    "\n",
    "            vecs = model.encoder(input_ids, attention_mask=attention_mask).last_hidden_state[:, 0].cpu().numpy()\n",
    "\n",
    "            for j, vec in enumerate(vecs):\n",
    "                original_idx = overflow_map[j].item()\n",
    "                all_embeddings.append(vec)\n",
    "                all_paper_ids.append(batch_ids[original_idx])\n",
    "\n",
    "    return all_paper_ids, np.vstack(all_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01c4ca92-a3a6-4393-8706-2d06d8464adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ids, paper_embeddings = encode_papers(model, df_collection, tokenizer)\n",
    "\n",
    "faiss_index = faiss.IndexFlatIP(paper_embeddings.shape[1])\n",
    "faiss_index.add(paper_embeddings)\n",
    "\n",
    "paper_id_map = {i: pid for i, pid in enumerate(paper_ids)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "132b6bf8-2fa4-4d1b-ad33-6ee1d813cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(model, df_query_dev, tokenizer, faiss_index, paper_id_map, topk=10):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in df_query_dev['tweet_text']:\n",
    "            enc = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "            tweet_vec = model.encoder(enc['input_ids'], attention_mask=enc['attention_mask']).last_hidden_state[:, 0]\n",
    "            tweet_vec = F.normalize(tweet_vec, dim=1).cpu().numpy()\n",
    "\n",
    "            D, I = faiss_index.search(tweet_vec, topk)\n",
    "            \n",
    "            preds = [paper_id_map[idx] for idx in I[0]]\n",
    "            predictions.append(preds)\n",
    "\n",
    "    df_query_dev['dense_topk'] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d70005d3-42a5-43fd-82ec-909ad80c3e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR Results: {1: 0.4492857142857143, 5: 0.5170714285714286, 10: 0.5267610544217688}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate retrieved candidates using MRR@k\n",
    "def get_performance_mrr(data, col_gold, col_pred, list_k = [1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(lambda x: (1/([i for i in x[col_pred][:k]].index(x[col_gold]) + 1) if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "        #performances.append(data[\"in_topx\"].mean())\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance\n",
    "\n",
    "# Evaluate\n",
    "retrieve(model, df_query_dev, tokenizer, faiss_index, paper_id_map)\n",
    "results_test = get_performance_mrr(df_query_dev, 'cord_uid', 'dense_topk')\n",
    "print(\"MRR Results:\", results_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca3378f-46f3-428b-9405-6287678bd264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_query_test['preds'] = df_query_test['dense_topk'].apply(lambda x: x[:5])\n",
    "df_query_test[['post_id', 'preds']].to_csv('predictions_new_scibert_final_context.tsv', index=None, sep='\\t')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
