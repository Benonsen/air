{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035b203c-b08b-4a09-9a9a-42554f4d814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "class ClaimSourceDataset(Dataset):\n",
    "    def __init__(self, df, collection_df, tokenizer, max_len=512):\n",
    "        self.df = df\n",
    "        self.collection = collection_df.set_index('cord_uid')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        tweet = row['tweet_text']\n",
    "        paper_id = row['cord_uid']\n",
    "        paper_row = self.collection.loc[paper_id]\n",
    "        paper = f\"{paper_row['title']} {paper_row['abstract']}\"\n",
    "\n",
    "        tweet_enc = self.tokenizer(tweet, truncation=True, padding='max_length', max_length=self.max_len, return_tensors=\"pt\")\n",
    "        paper_enc = self.tokenizer(paper, truncation=True, padding='max_length', max_length=self.max_len, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            'tweet_input_ids': tweet_enc['input_ids'].squeeze(),\n",
    "            'tweet_attention_mask': tweet_enc['attention_mask'].squeeze(),\n",
    "            'paper_input_ids': paper_enc['input_ids'].squeeze(),\n",
    "            'paper_attention_mask': paper_enc['attention_mask'].squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e5e995-d1f4-4f8c-b594-8e535e1a4fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "# this class can be used with any model, esp. with ones from the bert family,\n",
    "# therefore i/we did not create a new file for any model, we just changed the model_name var\n",
    "class EncoderModel(nn.Module):\n",
    "    def __init__(self, model_name=\"allenai/scibert_scivocab_uncased\"):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "    def forward(self, tweet_ids, tweet_mask, paper_ids, paper_mask):\n",
    "        tweet_vec = self.encoder(tweet_ids, attention_mask=tweet_mask).last_hidden_state[:, 0]\n",
    "        paper_vec = self.encoder(paper_ids, attention_mask=paper_mask).last_hidden_state[:, 0]\n",
    "        return tweet_vec, paper_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c39d5c8-27b1-494e-8aad-ea688fa1e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def contrastive_loss(tweet_vecs, paper_vecs, temperature=0.05):\n",
    "    tweet_vecs = F.normalize(tweet_vecs, dim=1)\n",
    "    paper_vecs = F.normalize(paper_vecs, dim=1)\n",
    "\n",
    "    logits = torch.matmul(tweet_vecs, paper_vecs.T) / temperature\n",
    "    labels = torch.arange(len(tweet_vecs)).to(tweet_vecs.device)\n",
    "    return F.cross_entropy(logits, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7152b6-ff4a-48d7-bede-07f079d252fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# prepare data for task\n",
    "\n",
    "# 1) Download the collection set from the Gitlab repository: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b\n",
    "# 2) Drag and drop the downloaded file to the \"Files\" section (left vertical menu on Colab)\n",
    "# 3) Modify the path to your local file path\n",
    "PATH_COLLECTION_DATA = 'subtask4b_collection_data.pkl' #MODIFY PATH\n",
    "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)\n",
    "\n",
    "PATH_QUERY_TRAIN_DATA = 'subtask4b_query_tweets_train.tsv' #MODIFY PATH\n",
    "PATH_QUERY_DEV_DATA = 'subtask4b_query_tweets_dev.tsv' #MODIFY PATH\n",
    "PATH_QUERY_DEV_TEST = 'subtask4b_query_tweets_test.tsv' #MODIFY PATH\n",
    "\n",
    "df_query_train = pd.read_csv(PATH_QUERY_TRAIN_DATA, sep = '\\t')\n",
    "df_query_dev = pd.read_csv(PATH_QUERY_DEV_DATA, sep = '\\t')\n",
    "df_query_test = pd.read_csv(PATH_QUERY_DEV_TEST, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695d8681-97b0-4898-8fd8-f5c5f478eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def compute_max_context_length(df_collection, df_query, max_rows=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "    lens = []\n",
    "    \n",
    "    paper_map = df_collection.set_index('cord_uid')[['title', 'abstract']].to_dict(orient='index')\n",
    "    \n",
    "    sample_rows = df_query if max_rows is None else df_query.sample(n=max_rows, random_state=42)\n",
    "\n",
    "    for _, row in sample_rows.iterrows():\n",
    "        tweet = str(row['tweet_text'])\n",
    "        cord_uid = row['cord_uid']\n",
    "        \n",
    "        if cord_uid in paper_map:\n",
    "            title = str(paper_map[cord_uid]['title'])\n",
    "            abstract = str(paper_map[cord_uid]['abstract'])\n",
    "            input_text = f\"{title} {abstract}\"\n",
    "            inputs = tokenizer.encode_plus(input_text, tweet, truncation=False, add_special_tokens=True)\n",
    "            lens.append(len(inputs['input_ids']))\n",
    "\n",
    "    if not lens:\n",
    "        print(\"No matching tweet-paper pairs found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Max length: {max(lens)} tokens\")\n",
    "    print(f\"Average length: {sum(lens)/len(lens):.2f} tokens\")\n",
    "    print(f\"95th percentile: {int(np.percentile(lens, 95))} tokens\")\n",
    "    \n",
    "\n",
    "compute_max_context_length(df_collection, df_query_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280be91d-7fab-46d6-86b4-6445543bf857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = EncoderModel().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "train_dataset = ClaimSourceDataset(df_query_train, df_collection, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346cbd24-b072-450c-9510-3bde88c0c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        tweet_ids = batch['tweet_input_ids'].to(device)\n",
    "        tweet_mask = batch['tweet_attention_mask'].to(device)\n",
    "        paper_ids = batch['paper_input_ids'].to(device)\n",
    "        paper_mask = batch['paper_attention_mask'].to(device)\n",
    "\n",
    "        tweet_vecs, paper_vecs = model(tweet_ids, tweet_mask, paper_ids, paper_mask)\n",
    "\n",
    "        loss = contrastive_loss(tweet_vecs, paper_vecs)\n",
    "   \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fdc2e8-b9e7-4846-a7ef-3e0a0a1c15f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def encode_papers(model, df_collection, tokenizer, batch_size=8):\n",
    "    model.eval()\n",
    "    paper_texts = df_collection.apply(lambda row: f\"{row['title']} {row['abstract']}\", axis=1).tolist()\n",
    "    paper_ids = df_collection['cord_uid'].tolist()\n",
    "\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(paper_texts), batch_size):\n",
    "            batch = paper_texts[i:i+batch_size]\n",
    "            encodings = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=256, return_overflowing_tokens=True)\n",
    "            input_ids = encodings['input_ids'].to(device)\n",
    "            attention_mask = encodings['attention_mask'].to(device)\n",
    "            vecs = model.encoder(input_ids, attention_mask=attention_mask).last_hidden_state[:, 0]\n",
    "            all_embeddings.append(vecs.cpu().numpy())\n",
    "\n",
    "    return paper_ids, np.vstack(all_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c4ca92-a3a6-4393-8706-2d06d8464adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ids, paper_embeddings = encode_papers(model, df_collection, tokenizer)\n",
    "\n",
    "faiss_index = faiss.IndexFlatIP(paper_embeddings.shape[1])\n",
    "faiss_index.add(paper_embeddings)\n",
    "\n",
    "paper_id_map = {i: pid for i, pid in enumerate(paper_ids)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132b6bf8-2fa4-4d1b-ad33-6ee1d813cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(model, df_query_dev, tokenizer, faiss_index, paper_id_map, topk=10):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in df_query_dev['tweet_text']:\n",
    "            enc = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "            tweet_vec = model.encoder(enc['input_ids'], attention_mask=enc['attention_mask']).last_hidden_state[:, 0]\n",
    "            tweet_vec = F.normalize(tweet_vec, dim=1).cpu().numpy()\n",
    "\n",
    "            D, I = faiss_index.search(tweet_vec, topk)\n",
    "            \n",
    "            preds = [paper_id_map[idx] for idx in I[0]]\n",
    "            predictions.append(preds)\n",
    "\n",
    "    df_query_dev['dense_topk'] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70005d3-42a5-43fd-82ec-909ad80c3e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate retrieved candidates using MRR@k\n",
    "def get_performance_mrr(data, col_gold, col_pred, list_k = [1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(lambda x: (1/([i for i in x[col_pred][:k]].index(x[col_gold]) + 1) if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "        #performances.append(data[\"in_topx\"].mean())\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance\n",
    "\n",
    "# Evaluate\n",
    "retrieve(model, df_query_dev, tokenizer, faiss_index, paper_id_map)\n",
    "results_test = get_performance_mrr(df_query_dev, 'cord_uid', 'dense_topk')\n",
    "print(\"MRR Results:\", results_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca3378f-46f3-428b-9405-6287678bd264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_query_test['preds'] = df_query_test['dense_topk'].apply(lambda x: x[:5])\n",
    "df_query_test[['post_id', 'preds']].to_csv('predictions_new_scibert_final_context.tsv', index=None, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8054168f-6ce1-4a1a-bf54-cbc993e624e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights\n",
    "model_save_path = \"scibert.pt\"\n",
    "torch.save(model.state_dict(), model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b40d1f-dbe9-43b1-a3ec-5f08fb0b76fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Reinitialize model and load weights\n",
    "model_loaded = EncoderModel()\n",
    "model_loaded.load_state_dict(torch.load(\"scibert.pt\"))\n",
    "model_loaded.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
