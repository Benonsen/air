{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "035b203c-b08b-4a09-9a9a-42554f4d814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class ClaimSourceDataset(Dataset):\n",
    "    def __init__(self, df, collection_df, tokenizer, max_len=256):\n",
    "        self.df = df\n",
    "        self.collection = collection_df.set_index('cord_uid')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        tweet = row['tweet_text']\n",
    "        paper_id = row['cord_uid']\n",
    "        paper_row = self.collection.loc[paper_id]\n",
    "        paper = f\"{paper_row['title']} {paper_row['abstract']}\"\n",
    "\n",
    "        tweet_enc = self.tokenizer(tweet, truncation=True, padding='max_length', max_length=self.max_len, return_tensors=\"pt\")\n",
    "        paper_enc = self.tokenizer(paper, truncation=True, padding='max_length', max_length=self.max_len, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            'tweet_input_ids': tweet_enc['input_ids'].squeeze(),\n",
    "            'tweet_attention_mask': tweet_enc['attention_mask'].squeeze(),\n",
    "            'paper_input_ids': paper_enc['input_ids'].squeeze(),\n",
    "            'paper_attention_mask': paper_enc['attention_mask'].squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9e5e995-d1f4-4f8c-b594-8e535e1a4fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class DualEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"scibert_scivocab_uncased\"):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "\n",
    "    def forward(self, tweet_ids, tweet_mask, paper_ids, paper_mask):\n",
    "        tweet_vec = self.encoder(tweet_ids, attention_mask=tweet_mask).last_hidden_state[:, 0]\n",
    "        paper_vec = self.encoder(paper_ids, attention_mask=paper_mask).last_hidden_state[:, 0]\n",
    "        return tweet_vec, paper_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c39d5c8-27b1-494e-8aad-ea688fa1e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def contrastive_loss(tweet_vecs, paper_vecs, temperature=0.05):\n",
    "    tweet_vecs = F.normalize(tweet_vecs, dim=1)\n",
    "    paper_vecs = F.normalize(paper_vecs, dim=1)\n",
    "\n",
    "    logits = torch.matmul(tweet_vecs, paper_vecs.T) / temperature\n",
    "    labels = torch.arange(len(tweet_vecs)).to(tweet_vecs.device)\n",
    "    return F.cross_entropy(logits, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb7152b6-ff4a-48d7-bede-07f079d252fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60931a9d-c3b2-46f9-8926-681b68c5ce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Download the collection set from the Gitlab repository: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b\n",
    "# 2) Drag and drop the downloaded file to the \"Files\" section (left vertical menu on Colab)\n",
    "# 3) Modify the path to your local file path\n",
    "PATH_COLLECTION_DATA = 'subtask4b_collection_data.pkl' #MODIFY PATH\n",
    "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959f6a4d-9466-49e7-9b4b-c171f39b04fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98d4d9a6-9f2d-4331-87e4-8bcd681d6c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_QUERY_TRAIN_DATA = 'subtask4b_query_tweets_train.tsv' #MODIFY PATH\n",
    "PATH_QUERY_DEV_DATA = 'subtask4b_query_tweets_dev.tsv' #MODIFY PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a23460e0-d2ca-48d8-b7a2-7111841bed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_train = pd.read_csv(PATH_QUERY_TRAIN_DATA, sep = '\\t')\n",
    "df_query_dev = pd.read_csv(PATH_QUERY_DEV_DATA, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280be91d-7fab-46d6-86b4-6445543bf857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "346cbd24-b072-450c-9510-3bde88c0c989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 804/804 [19:13<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.2893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 804/804 [19:15<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.1295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 804/804 [19:16<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.0693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = DualEncoder().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "train_dataset = ClaimSourceDataset(df_query_train, df_collection, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        tweet_ids = batch['tweet_input_ids'].to(device)\n",
    "        tweet_mask = batch['tweet_attention_mask'].to(device)\n",
    "        paper_ids = batch['paper_input_ids'].to(device)\n",
    "        paper_mask = batch['paper_attention_mask'].to(device)\n",
    "\n",
    "        tweet_vecs, paper_vecs = model(tweet_ids, tweet_mask, paper_ids, paper_mask)\n",
    "\n",
    "        loss = contrastive_loss(tweet_vecs, paper_vecs)\n",
    "   \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0fdc2e8-b9e7-4846-a7ef-3e0a0a1c15f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def encode_papers(model, df_collection, tokenizer, batch_size=64):\n",
    "    model.eval()\n",
    "    paper_texts = df_collection.apply(lambda row: f\"{row['title']} {row['abstract']}\", axis=1).tolist()\n",
    "    paper_ids = df_collection['cord_uid'].tolist()\n",
    "\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(paper_texts), batch_size):\n",
    "            batch = paper_texts[i:i+batch_size]\n",
    "            encodings = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=256)\n",
    "            input_ids = encodings['input_ids'].to(device)\n",
    "            attention_mask = encodings['attention_mask'].to(device)\n",
    "            vecs = model.encoder(input_ids, attention_mask=attention_mask).last_hidden_state[:, 0]\n",
    "            all_embeddings.append(vecs.cpu().numpy())\n",
    "\n",
    "    return paper_ids, np.vstack(all_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01c4ca92-a3a6-4393-8706-2d06d8464adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ids, paper_embeddings = encode_papers(model, df_collection, tokenizer)\n",
    "\n",
    "faiss_index = faiss.IndexFlatIP(paper_embeddings.shape[1])\n",
    "faiss_index.add(paper_embeddings)\n",
    "\n",
    "paper_id_map = {i: pid for i, pid in enumerate(paper_ids)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "132b6bf8-2fa4-4d1b-ad33-6ee1d813cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(model, df_query_dev, tokenizer, faiss_index, paper_id_map, topk=5):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in df_query_dev['tweet_text']:\n",
    "            enc = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(device)\n",
    "            tweet_vec = model.encoder(enc['input_ids'], attention_mask=enc['attention_mask']).last_hidden_state[:, 0]\n",
    "            tweet_vec = F.normalize(tweet_vec, dim=1).cpu().numpy()\n",
    "\n",
    "            D, I = faiss_index.search(tweet_vec, topk)\n",
    "            preds = [paper_id_map[idx] for idx in I[0]]\n",
    "            predictions.append(preds)\n",
    "\n",
    "    df_query_dev['dense_topk'] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d70005d3-42a5-43fd-82ec-909ad80c3e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR Results: {1: 0.5092857142857142, 5: 0.5843452380952381}\n"
     ]
    }
   ],
   "source": [
    "def get_performance_mrr(data, col_gold, col_pred, list_k = [1, 5]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(lambda x: (1/([i for i in x[col_pred][:k]].index(x[col_gold]) + 1) if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance\n",
    "\n",
    "# Evaluate\n",
    "retrieve(model, df_query_dev, tokenizer, faiss_index, paper_id_map)\n",
    "results_dev = get_performance_mrr(df_query_dev, 'cord_uid', 'dense_topk', list_k=[1, 5])\n",
    "print(\"MRR Results:\", results_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ca3378f-46f3-428b-9405-6287678bd264",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_dev['preds'] = df_query_dev['dense_topk'].apply(lambda x: x[:5])\n",
    "df_query_dev[['post_id', 'preds']].to_csv('predictions_scibert.tsv', index=False, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e7425ff-2904-403b-a519-ffbb07c68efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"out_scibert.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a30aeda-ddf2-4aa0-881c-31e43a73edd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cord_uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Oral care in rehabilitation medicine: oral vul...</td>\n",
       "      <td>htlvpvz5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this study isn't receiving sufficient attentio...</td>\n",
       "      <td>4kfl29ul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>thanks, xi jinping. a reminder that this study...</td>\n",
       "      <td>jtwb17u8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Taiwan - a population of 23 million has had ju...</td>\n",
       "      <td>0w9k8iy1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Obtaining a diagnosis of autism in lower incom...</td>\n",
       "      <td>tiqksd69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12848</th>\n",
       "      <td>14248</td>\n",
       "      <td>\"evidence on covid-19 reveals a growing body o...</td>\n",
       "      <td>9169o29b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12849</th>\n",
       "      <td>14249</td>\n",
       "      <td>Outdoor lighting has detrimental impacts on lo...</td>\n",
       "      <td>s2bpha8l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12850</th>\n",
       "      <td>14250</td>\n",
       "      <td>26/ and influenza virus (and other pathogens, ...</td>\n",
       "      <td>atloc9th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12851</th>\n",
       "      <td>14251</td>\n",
       "      <td>does it?'sars-cov-2-naïve vaccinees had a 13.0...</td>\n",
       "      <td>t4y1ylb3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12852</th>\n",
       "      <td>14252</td>\n",
       "      <td>when \"the airway immune cells of children are ...</td>\n",
       "      <td>nlsv8bin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12853 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       post_id                                         tweet_text  cord_uid\n",
       "0            0  Oral care in rehabilitation medicine: oral vul...  htlvpvz5\n",
       "1            1  this study isn't receiving sufficient attentio...  4kfl29ul\n",
       "2            2  thanks, xi jinping. a reminder that this study...  jtwb17u8\n",
       "3            3  Taiwan - a population of 23 million has had ju...  0w9k8iy1\n",
       "4            4  Obtaining a diagnosis of autism in lower incom...  tiqksd69\n",
       "...        ...                                                ...       ...\n",
       "12848    14248  \"evidence on covid-19 reveals a growing body o...  9169o29b\n",
       "12849    14249  Outdoor lighting has detrimental impacts on lo...  s2bpha8l\n",
       "12850    14250  26/ and influenza virus (and other pathogens, ...  atloc9th\n",
       "12851    14251  does it?'sars-cov-2-naïve vaccinees had a 13.0...  t4y1ylb3\n",
       "12852    14252  when \"the airway immune cells of children are ...  nlsv8bin\n",
       "\n",
       "[12853 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_query_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8054168f-6ce1-4a1a-bf54-cbc993e624e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
